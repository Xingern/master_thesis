{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network for stable points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read & manipulate data\n",
    "import keras_tuner as kt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU since slow on my machine\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight') #plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "plt.rcParams['figure.facecolor'] = 'white'  # For the figure background\n",
    "plt.rcParams['axes.facecolor'] = 'white'    # For the axes background\n",
    "plt.rcParams['axes.edgecolor'] = 'white'    # Set the axes edge color to white\n",
    "plt.rcParams['savefig.facecolor'] = 'white' # For the figure background when saving\n",
    "\n",
    "# load the dataset\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "# misc\n",
    "import random as rn\n",
    "import utils\n",
    "\n",
    "# setting random seeds for libraries to ensure reproducibility\n",
    "RANDOM_SEED = 420\n",
    "TRAINING_SAMPLE = 15000\n",
    "VALIDATE_SIZE = 0.2\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "pkl_path = '/Users/junxingli/Desktop/master_thesis/data/processed/SRD_Lysekil.pkl'\n",
    "tuner_path = '/Users/junxingli/Desktop/master_thesis/models/ann/'\n",
    "proj_name = 'tune_hypermodel-Vslett/'\n",
    "best_model_path = tuner_path + proj_name + 'best_model'\n",
    "\n",
    "# Figure paths\n",
    "overwrite = False\n",
    "figs_path = '/Users/junxingli/Desktop/master_thesis/figs/ann/'\n",
    "plots_path = figs_path + proj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/processed/SRD_Lysekil.pkl\")\n",
    "df = df[df['Status'] == 'Stable']\n",
    "\n",
    "#X = df.drop(['Status', 'SRD', 'A11', 'B11', 'A12', 'B12', 'Cap', 'L7'], axis=1)\n",
    "X = df.drop(['Status', 'SRD', 'T5'], axis=1)\n",
    "y = df[['Time', 'SRD']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Standard scaling the data\n",
    "scaler = StandardScaler()\n",
    "cols = [col for col in X_train.columns if col != 'Time']\n",
    "X_train[cols] = scaler.fit_transform(X_train[cols])\n",
    "X_test[cols] = scaler.transform(X_test[cols])\n",
    "\n",
    "# Need to reshape since the scaler expects a 2D array\n",
    "y_train['SRD'] = scaler.fit_transform(y_train['SRD'].to_numpy().reshape(-1, 1))\n",
    "y_test['SRD'] = scaler.transform(y_test['SRD'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "input_dim = X.drop('Time', axis=1).shape[1]\n",
    "output_dim = y.drop('Time', axis=1).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df_res):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_res[\"Time\"], \n",
    "            y=df_res[\"Actual\"], \n",
    "            mode='markers', \n",
    "            marker=dict(size=4),\n",
    "            name='Actual',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_res[\"Time\"], \n",
    "            y=df_res[\"Predicted\"], \n",
    "            mode='markers', \n",
    "            marker=dict(size=4),\n",
    "            name='Prediction',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Modelling the SRD using ANN\", \n",
    "                    legend=dict(\n",
    "                            orientation=\"h\",\n",
    "                            yanchor=\"bottom\",\n",
    "                            y=1.02,\n",
    "                            xanchor=\"right\",\n",
    "                            x=0.3\n",
    "                        ),\n",
    "                    template=\"seaborn\",\n",
    "                    xaxis_title=\"Time\", \n",
    "                    yaxis_title=\"Specific Reboiler Duty [MJ/kgCO2]\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def prediction_df(model, scaler, X, y):\n",
    "    y_pred = model.predict(X.drop('Time', axis=1))\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    y_inv = y.copy()\n",
    "    y_inv['SRD'] = scaler.inverse_transform(y['SRD'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    df_res = pd.DataFrame({'Time': y_inv['Time'], \n",
    "                           'Actual': y_inv['SRD'], \n",
    "                           'Predicted': y_pred.flatten()})\n",
    "    df_res.sort_values('Time')\n",
    "    \n",
    "    rmse = mean_squared_error(df_res['Actual'], df_res['Predicted'], squared=False)\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    \n",
    "    return df_res, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple ANN with zero hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(optimizer='adam'):\n",
    "    classifier = Sequential()\n",
    "\n",
    "    classifier.add(Input(shape=(input_dim,)))    \n",
    "    #classifier.add(Dense(units=128, activation='relu'))\n",
    "    #classifier.add(Dense(units=128, activation='relu'))\n",
    "    classifier.add(Dense(units=1, activation='linear'))\n",
    "    \n",
    "    classifier.compile(loss='mean_squared_error', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mean_squared_error'])\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "regressor = buildModel('adam')\n",
    "History = regressor.fit(x=X_train.drop('Time', axis=1), \n",
    "                        y=y_train.drop('Time', axis=1),\n",
    "                        batch_size=32, \n",
    "                        epochs=100, \n",
    "                        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights = regressor.layers[0].get_weights()[0]\n",
    "\n",
    "df = pd.DataFrame({'Feature': X.drop('Time', axis=1).columns,\n",
    "                   'Coefficient': first_layer_weights.flatten()})\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(x=df['Feature'], y=df['Coefficient'])])  \n",
    "\n",
    "fig.update_layout(\n",
    "                xaxis_title=\"Feature\",\n",
    "                yaxis_title=\"Coefficient\",\n",
    "                height=400,\n",
    "                width=800,\n",
    "                legend=dict(\n",
    "                            orientation=\"h\",\n",
    "                            yanchor=\"bottom\",\n",
    "                            y=1.02,\n",
    "                            xanchor=\"right\",\n",
    "                            x=0.15\n",
    "                        ),\n",
    "                template=\"seaborn\",\n",
    "                margin=dict(l=0, r=20, t=20, b=10),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(History.history['mean_squared_error'])\n",
    "plt.title('Loss Function Over Epochs')\n",
    "plt.ylabel('MSE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(df_res):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_res[\"Time\"], \n",
    "            y=df_res[\"Actual\"], \n",
    "            mode='markers', \n",
    "            marker=dict(size=4),\n",
    "            name='Actual',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_res[\"Time\"], \n",
    "            y=df_res[\"Predicted\"], \n",
    "            mode='markers', \n",
    "            marker=dict(size=4),\n",
    "            name='Prediction',\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Modelling the SRD using ANN\", \n",
    "                    legend=dict(\n",
    "                            orientation=\"h\",\n",
    "                            yanchor=\"bottom\",\n",
    "                            y=1.02,\n",
    "                            xanchor=\"right\",\n",
    "                            x=0.3\n",
    "                        ),\n",
    "                    template=\"seaborn\",\n",
    "                    xaxis_title=\"Time\", \n",
    "                    yaxis_title=\"Specific Reboiler Duty [MJ/kgCO2]\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def prediction_df(model, scaler, X, y):\n",
    "    y_pred = model.predict(X.drop('Time', axis=1))\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    y_inv = y.copy()\n",
    "    y_inv['SRD'] = scaler.inverse_transform(y['SRD'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    df_res = pd.DataFrame({'Time': y_inv['Time'], \n",
    "                           'Actual': y_inv['SRD'], \n",
    "                           'Predicted': y_pred.flatten()})\n",
    "    df_res.sort_values('Time')\n",
    "    \n",
    "    rmse = mean_squared_error(df_res['Actual'], df_res['Predicted'], squared=False)\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = prediction_df(regressor, scaler, X_train, y_train)\n",
    "df_test = prediction_df(regressor, scaler, X_test, y_test)\n",
    "#plot_results(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
    "\n",
    "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp): \n",
    "        hp_learning_rate = hp.Choice('learning_rate', \n",
    "                                      values=[1e-1, 1e-2, 1e-3])\n",
    "            \n",
    "        model = keras.Sequential()\n",
    "        model.add(Input(shape=(input_dim,)))\n",
    "        for i in range(hp.Int('num_layers', 0, 4)):\n",
    "            model.add(Dense(units=hp.Int('units_' + str(i), \n",
    "                                         min_value=50, \n",
    "                                         max_value=150, \n",
    "                                         step=5),\n",
    "                            activation='relu'))\n",
    "        \n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Select optimizer    \n",
    "        optimizer = hp.Choice('optimizer', values=['adam', 'SGD', 'rmsprop'])\n",
    "        if optimizer == 'adam':\n",
    "            model.compile(\n",
    "            optimizer=keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "            )\n",
    "        elif optimizer == 'SGD':\n",
    "            model.compile(\n",
    "            optimizer=keras.optimizers.legacy.SGD(learning_rate=hp_learning_rate),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "            )\n",
    "        elif optimizer == 'rmsprop':\n",
    "            model.compile(\n",
    "            optimizer=keras.optimizers.legacy.RMSprop(learning_rate=hp_learning_rate),\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error']\n",
    "            )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    MyHyperModel(),\n",
    "    objective='val_mean_squared_error',\n",
    "    max_epochs=1000,\n",
    "    overwrite=False,\n",
    "    directory=tuner_path,\n",
    "    project_name=proj_name)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mean_squared_error', \n",
    "                                              patience=25)\n",
    "\n",
    "tuner.search(X_train.drop('Time', axis=1),\n",
    "             y_train.drop('Time', axis=1), \n",
    "             epochs=500, \n",
    "             validation_split=0.25, \n",
    "             callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "History = best_model.fit(X_train.drop('Time', axis=1), \n",
    "                         y_train.drop('Time', axis=1), \n",
    "                         epochs=1000, \n",
    "                         callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                                    monitor='mean_squared_error', \n",
    "                                    patience=50)\n",
    "                                    ],\n",
    "                         validation_split=0)\n",
    "\n",
    "best_model.save(best_model_path)\n",
    "\n",
    "best_model.summary()\n",
    "print(f\"\"\"{best_hps.values}\n",
    "-------------------------------------\n",
    "Model saved to {best_model_path}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(History.history['mean_squared_error'], linewidth=1)\n",
    "plt.title('Loss Function Over Epochs')\n",
    "plt.ylabel('MSE value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(best_model_path)\n",
    "df_train, rmse_train = prediction_df(loaded_model, scaler, X_train, y_train)\n",
    "df_test, rmse_test = prediction_df(loaded_model, scaler, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histograms(df):\n",
    "    x = df['Actual']\n",
    "    lower_percentile = np.percentile(x, 0.5)\n",
    "    upper_percentile = np.percentile(x, 99.5)\n",
    "    mask = (x > lower_percentile) & (x < upper_percentile)\n",
    "    x_0_5 = x[mask.values].squeeze()\n",
    "    x_0_5_res = x_0_5 - df['Predicted'][mask.values].squeeze()\n",
    "    \n",
    "    q25, q75 = np.percentile(x_0_5, [25, 75])\n",
    "    bin_width = 2 * (q75 - q25) * len(x_0_5) ** (-1/3)\n",
    "    bins = round((x_0_5.max() - x_0_5.min()) / bin_width)\n",
    "    print(\"Freedman-Diaconis number of bins:\", bins)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    ax1.hist(x_0_5, bins=bins, edgecolor='black', alpha=0.8)\n",
    "    ax1.set_xlabel('Specific Reboiler Duty [MJ/kg CO2]')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    ax2.hist(x_0_5_res, bins=bins, edgecolor='black', alpha=0.8)\n",
    "    ax2.set_xlabel('Residual [MJ/kg CO2]')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "generate_histograms(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity plot\n",
    "limit = np.array([3.5, 4.6])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.plot(limit, limit, color='black', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 1.01*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 0.99*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.1, 1.01*limit[1]-0.18, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=45)\n",
    "plt.text(limit[1]-0.05, 0.99*limit[1]-0.09, '-1% error', verticalalignment='top', horizontalalignment='right', color='black', rotation=45)\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(limit)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "utils.save_file(plt.gcf(), plots_path, \"Parity_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Residuals plot\n",
    "mean = df['SRD'].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axhline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axhline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.07, 0.0105*mean, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black')\n",
    "plt.text(limit[1]-0.07, -0.0105*mean, '-1% error', verticalalignment='top', horizontalalignment='right', color='black')\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(-0.15, 0.2)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "utils.save_file(plt.gcf(), plots_path, \"Residuals_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Residuals histogram plot\n",
    "mean = df['SRD'].mean()\n",
    "x_lim = np.array([-0.2, 0.2])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = plt.gca()\n",
    "yticks = ax.yaxis.get_major_ticks() \n",
    "yticks[0].label1.set_visible(False)\n",
    "\n",
    "sns.histplot(df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9)\n",
    "sns.histplot(df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.8)\n",
    "\n",
    "plt.text(0.011*mean, 500, '+1% error', verticalalignment='bottom', horizontalalignment='left', color='black', rotation=90)\n",
    "plt.text(-0.01*mean, 500, '-1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=90)\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axvline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axvline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5, loc='upper left', shadow=True, fancybox=True)\n",
    "\n",
    "utils.save_file(plt.gcf(), plots_path, \"ResidualHistogram_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Density plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.kdeplot(df_train['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Actual')\n",
    "sns.kdeplot(df_train['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Predicted')\n",
    "axs[0].legend(loc='upper left', shadow=True, fancybox=True)\n",
    "axs[0].set_xlim(3.5, 4.5)\n",
    "axs[0].set_title('Training')\n",
    "\n",
    "sns.kdeplot(df_test['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Actual')\n",
    "sns.kdeplot(df_test['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Predicted')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlim(3.5, 4.5)\n",
    "axs[1].set_title('Testing')\n",
    "\n",
    "utils.save_file(plt.gcf(), plots_path, \"KDE_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFS + ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read & manipulate data\n",
    "import keras_tuner as kt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU since slow on my machine\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "# load the dataset\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "\n",
    "# misc\n",
    "import random as rn\n",
    "\n",
    "# setting random seeds for libraries to ensure reproducibility\n",
    "RANDOM_SEED = 420\n",
    "TRAINING_SAMPLE = 15000\n",
    "VALIDATE_SIZE = 0.2\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "pkl_path = '/Users/junxingli/Desktop/master_thesis/data/processed/SRD_Lysekil.pkl'\n",
    "model_path = '/Users/junxingli/Desktop/master_thesis/models/ann_sfs/' \n",
    "feat_path = '3_features'\n",
    "tuner_path = os.path.join(model_path, feat_path)\n",
    "file_path = os.path.join(model_path, 'sfs_results.pkl')\n",
    "\n",
    "df = pd.read_pickle(pkl_path)\n",
    "df = df[df['Status'] == 'Stable']\n",
    "features = df.drop(['Status', 'SRD', 'T5', 'Time'], axis=1).columns.to_list()\n",
    "\n",
    "def update_dataframe(df, selected_features, training_error, test_error):\n",
    "    \"\"\"\n",
    "    Update the DataFrame with a new row containing information about the selected features, training error, and test error.\n",
    "    \n",
    "    Parameters:\n",
    "    selected_features (list): A list of selected features for this round.\n",
    "    training_error (float): The training error for this round.\n",
    "    test_error (float): The test error for this round.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The updated DataFrame with the new row appended.\n",
    "    \"\"\"\n",
    "    # Create a new row with 0s for all features\n",
    "    features = df.columns.to_list()[:-2]\n",
    "    new_row = {feature: 0 for feature in features}\n",
    "    \n",
    "    # Update the row based on the selected features for this round\n",
    "    for feature in selected_features:\n",
    "        new_row[feature] = 1\n",
    "        \n",
    "    # Add training and test error\n",
    "    new_row['Training Error'] = training_error\n",
    "    new_row['Test Error'] = test_error\n",
    "    \n",
    "    # Append the new row to the DataFrame\n",
    "    df.loc[len(df.index)] = new_row\n",
    "    return df\n",
    "\n",
    "def get_rmse(model, scaler, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    \n",
    "    y_inv = y.copy()\n",
    "    y_inv['SRD'] = scaler.inverse_transform(y['SRD'].to_numpy().reshape(-1, 1))\n",
    "    rmse = mean_squared_error(y_inv['SRD'], y_pred.flatten(), squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def tune_ANN(df, X_features, proj_path, update=False):\n",
    "    X = df[X_features]\n",
    "    y = df[['SRD']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.2)\n",
    "    \n",
    "    input_dim = X.shape[1]\n",
    "    output_dim = y.shape[1]\n",
    "    class MyHyperModel(kt.HyperModel):\n",
    "        def build(self, hp): \n",
    "            hp_learning_rate = hp.Choice('learning_rate', \n",
    "                                        values=[1e-1, 1e-2])\n",
    "                \n",
    "            model = keras.Sequential()\n",
    "            model.add(Input(shape=(input_dim,)))\n",
    "            for i in range(hp.Int('num_layers', 0, 3)):\n",
    "                model.add(Dense(units=hp.Int('units_' + str(i), \n",
    "                                            min_value=5, \n",
    "                                            max_value=40, \n",
    "                                            step=5),\n",
    "                                activation='relu'))\n",
    "            \n",
    "            model.add(Dense(output_dim, activation='linear'))\n",
    "            \n",
    "            # Select optimizer    \n",
    "            optimizer = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
    "            if optimizer == 'adam':\n",
    "                model.compile(\n",
    "                optimizer=keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mean_squared_error']\n",
    "                )\n",
    "            elif optimizer == 'SGD':\n",
    "                model.compile(\n",
    "                optimizer=keras.optimizers.legacy.SGD(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mean_squared_error']\n",
    "                )\n",
    "            elif optimizer == 'rmsprop':\n",
    "                model.compile(\n",
    "                optimizer=keras.optimizers.legacy.RMSprop(learning_rate=hp_learning_rate),\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mean_squared_error']\n",
    "                )\n",
    "            \n",
    "            return model\n",
    "\n",
    "        def fit(self, hp, model, *args, **kwargs):\n",
    "            return model.fit(\n",
    "                *args,\n",
    "                batch_size=hp.Choice(\"batch_size\", [32, 64, 128]),\n",
    "                **kwargs,\n",
    "            )\n",
    "    \n",
    "    # Standard scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_train['SRD'] = scaler.fit_transform(y_train['SRD'].to_numpy().reshape(-1, 1))\n",
    "    y_test['SRD'] = scaler.transform(y_test['SRD'].to_numpy().reshape(-1, 1))\n",
    "    \n",
    "    metric = 'val_mean_squared_error'\n",
    "    tuner = kt.Hyperband(MyHyperModel(),\n",
    "                         objective=metric,\n",
    "                         max_epochs=200,\n",
    "                         overwrite=False,\n",
    "                         directory=tuner_path,\n",
    "                         project_name=proj_path)\n",
    "\n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor=metric, \n",
    "                                                patience=15)\n",
    "\n",
    "    tuner.search(X_train, y_train, epochs=200, \n",
    "                 validation_split=0.25, callbacks=[stop_early])\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    History = best_model.fit(X_train, \n",
    "                            y_train, \n",
    "                            epochs=800, \n",
    "                            callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                                        monitor='mean_squared_error', \n",
    "                                        patience=50)\n",
    "                                        ],\n",
    "                            validation_split=0)\n",
    "    \n",
    "    rmse_train = get_rmse(best_model, scaler, X_train, y_train)\n",
    "    rmse_test = get_rmse(best_model, scaler, X_test, y_test)\n",
    "    \n",
    "    # Save the model architecture and hyperparameters as txt-file\n",
    "    summary_dir = os.path.join(tuner_path, proj_path, '00summary')\n",
    "    os.makedirs(summary_dir, exist_ok=True)\n",
    "    filename = 'model_summary.txt'\n",
    "    summary_path = os.path.join(summary_dir, filename)\n",
    "\n",
    "    with open(summary_path, 'w') as f:\n",
    "        txt = f\"\"\"Training error: {rmse_train}\n",
    "        Test error: {rmse_test}\n",
    "        Parameters: {X_features}\n",
    "        \"\"\"\n",
    "        f.write(str(txt))\n",
    "        best_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "        \n",
    "        if hasattr(best_hps, 'values'):\n",
    "            for param, value in best_hps.values.items():\n",
    "                f.write(f\"{param}: {value}\\n\")\n",
    "        else:\n",
    "            f.write(str(best_hps))\n",
    "            \n",
    "    # Save the model feautures and errors to a DataFrame\n",
    "    if update:\n",
    "        if not os.path.exists(file_path):\n",
    "            features = df.drop(['Status', 'SRD', 'T5', 'Time'], axis=1).columns.to_list()\n",
    "            sfs_df = pd.DataFrame(columns=features + ['Training Error', 'Test Error'])\n",
    "            sfs_df.to_pickle(file_path)\n",
    "        \n",
    "        sfs_df = pd.read_pickle(file_path)    \n",
    "        sfs_df = update_dataframe(sfs_df, X_features, rmse_train, rmse_test)\n",
    "        sfs_df.to_pickle(file_path)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 03s]\n",
      "val_mean_squared_error: 0.8120920658111572\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.19516588747501373\n",
      "Total elapsed time: 00h 07m 24s\n",
      "Epoch 1/800\n",
      "299/299 [==============================] - 0s 474us/step - loss: 0.3185 - mean_squared_error: 0.3185\n",
      "Epoch 2/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.3171 - mean_squared_error: 0.3171\n",
      "Epoch 3/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.3135 - mean_squared_error: 0.3135\n",
      "Epoch 4/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.3169 - mean_squared_error: 0.3169\n",
      "Epoch 5/800\n",
      "299/299 [==============================] - 0s 441us/step - loss: 0.3159 - mean_squared_error: 0.3159\n",
      "Epoch 6/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.3142 - mean_squared_error: 0.3142\n",
      "Epoch 7/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.3170 - mean_squared_error: 0.3170\n",
      "Epoch 8/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.3168 - mean_squared_error: 0.3168\n",
      "Epoch 9/800\n",
      "299/299 [==============================] - 0s 463us/step - loss: 0.3112 - mean_squared_error: 0.3112\n",
      "Epoch 10/800\n",
      "299/299 [==============================] - 0s 440us/step - loss: 0.3118 - mean_squared_error: 0.3118\n",
      "Epoch 11/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.3091 - mean_squared_error: 0.3091\n",
      "Epoch 12/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.3107 - mean_squared_error: 0.3107\n",
      "Epoch 13/800\n",
      "299/299 [==============================] - 0s 462us/step - loss: 0.3066 - mean_squared_error: 0.3066\n",
      "Epoch 14/800\n",
      "299/299 [==============================] - 0s 463us/step - loss: 0.3101 - mean_squared_error: 0.3101\n",
      "Epoch 15/800\n",
      "299/299 [==============================] - 0s 475us/step - loss: 0.3080 - mean_squared_error: 0.3080\n",
      "Epoch 16/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.3089 - mean_squared_error: 0.3089\n",
      "Epoch 17/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.3097 - mean_squared_error: 0.3097\n",
      "Epoch 18/800\n",
      "299/299 [==============================] - 0s 441us/step - loss: 0.3070 - mean_squared_error: 0.3070\n",
      "Epoch 19/800\n",
      "299/299 [==============================] - 0s 439us/step - loss: 0.3046 - mean_squared_error: 0.3046\n",
      "Epoch 20/800\n",
      "299/299 [==============================] - 0s 440us/step - loss: 0.3055 - mean_squared_error: 0.3055\n",
      "Epoch 21/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.3069 - mean_squared_error: 0.3069\n",
      "Epoch 22/800\n",
      "299/299 [==============================] - 0s 909us/step - loss: 0.3051 - mean_squared_error: 0.3051\n",
      "Epoch 23/800\n",
      "299/299 [==============================] - 0s 468us/step - loss: 0.3040 - mean_squared_error: 0.3040\n",
      "Epoch 24/800\n",
      "299/299 [==============================] - 0s 461us/step - loss: 0.3013 - mean_squared_error: 0.3013\n",
      "Epoch 25/800\n",
      "299/299 [==============================] - 0s 470us/step - loss: 0.3066 - mean_squared_error: 0.3066\n",
      "Epoch 26/800\n",
      "299/299 [==============================] - 0s 472us/step - loss: 0.3039 - mean_squared_error: 0.3039\n",
      "Epoch 27/800\n",
      "299/299 [==============================] - 0s 462us/step - loss: 0.3014 - mean_squared_error: 0.3014\n",
      "Epoch 28/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2994 - mean_squared_error: 0.2994\n",
      "Epoch 29/800\n",
      "299/299 [==============================] - 0s 488us/step - loss: 0.3009 - mean_squared_error: 0.3009\n",
      "Epoch 30/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.3017 - mean_squared_error: 0.3017\n",
      "Epoch 31/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.3038 - mean_squared_error: 0.3038\n",
      "Epoch 32/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2994 - mean_squared_error: 0.2994\n",
      "Epoch 33/800\n",
      "299/299 [==============================] - 0s 461us/step - loss: 0.2984 - mean_squared_error: 0.2984\n",
      "Epoch 34/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.3017 - mean_squared_error: 0.3017\n",
      "Epoch 35/800\n",
      "299/299 [==============================] - 0s 467us/step - loss: 0.2989 - mean_squared_error: 0.2989\n",
      "Epoch 36/800\n",
      "299/299 [==============================] - 0s 462us/step - loss: 0.2986 - mean_squared_error: 0.2986\n",
      "Epoch 37/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2980 - mean_squared_error: 0.2980\n",
      "Epoch 38/800\n",
      "299/299 [==============================] - 0s 459us/step - loss: 0.2969 - mean_squared_error: 0.2969\n",
      "Epoch 39/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2987 - mean_squared_error: 0.2987\n",
      "Epoch 40/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2975 - mean_squared_error: 0.2975\n",
      "Epoch 41/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2977 - mean_squared_error: 0.2977\n",
      "Epoch 42/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2976 - mean_squared_error: 0.2976\n",
      "Epoch 43/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2973 - mean_squared_error: 0.2973\n",
      "Epoch 44/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2951 - mean_squared_error: 0.2951\n",
      "Epoch 45/800\n",
      "299/299 [==============================] - 0s 461us/step - loss: 0.2993 - mean_squared_error: 0.2993\n",
      "Epoch 46/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2968 - mean_squared_error: 0.2968\n",
      "Epoch 47/800\n",
      "299/299 [==============================] - 0s 809us/step - loss: 0.2983 - mean_squared_error: 0.2983\n",
      "Epoch 48/800\n",
      "299/299 [==============================] - 0s 572us/step - loss: 0.2980 - mean_squared_error: 0.2980\n",
      "Epoch 49/800\n",
      "299/299 [==============================] - 0s 578us/step - loss: 0.2964 - mean_squared_error: 0.2964\n",
      "Epoch 50/800\n",
      "299/299 [==============================] - 0s 447us/step - loss: 0.2960 - mean_squared_error: 0.2960\n",
      "Epoch 51/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2954 - mean_squared_error: 0.2954\n",
      "Epoch 52/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2961 - mean_squared_error: 0.2961\n",
      "Epoch 53/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2992 - mean_squared_error: 0.2992\n",
      "Epoch 54/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2937 - mean_squared_error: 0.2937\n",
      "Epoch 55/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2943 - mean_squared_error: 0.2943\n",
      "Epoch 56/800\n",
      "299/299 [==============================] - 0s 458us/step - loss: 0.2940 - mean_squared_error: 0.2940\n",
      "Epoch 57/800\n",
      "299/299 [==============================] - 0s 442us/step - loss: 0.2965 - mean_squared_error: 0.2965\n",
      "Epoch 58/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2983 - mean_squared_error: 0.2983\n",
      "Epoch 59/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2939 - mean_squared_error: 0.2939\n",
      "Epoch 60/800\n",
      "299/299 [==============================] - 0s 429us/step - loss: 0.2965 - mean_squared_error: 0.2965\n",
      "Epoch 61/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2970 - mean_squared_error: 0.2970\n",
      "Epoch 62/800\n",
      "299/299 [==============================] - 0s 466us/step - loss: 0.2970 - mean_squared_error: 0.2970\n",
      "Epoch 63/800\n",
      "299/299 [==============================] - 0s 439us/step - loss: 0.2944 - mean_squared_error: 0.2944\n",
      "Epoch 64/800\n",
      "299/299 [==============================] - 0s 472us/step - loss: 0.2943 - mean_squared_error: 0.2943\n",
      "Epoch 65/800\n",
      "299/299 [==============================] - 0s 485us/step - loss: 0.2951 - mean_squared_error: 0.2951\n",
      "Epoch 66/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2947 - mean_squared_error: 0.2947\n",
      "Epoch 67/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2960 - mean_squared_error: 0.2960\n",
      "Epoch 68/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2950 - mean_squared_error: 0.2950\n",
      "Epoch 69/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2967 - mean_squared_error: 0.2967\n",
      "Epoch 70/800\n",
      "299/299 [==============================] - 0s 463us/step - loss: 0.2965 - mean_squared_error: 0.2965\n",
      "Epoch 71/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2952 - mean_squared_error: 0.2952\n",
      "Epoch 72/800\n",
      "299/299 [==============================] - 0s 465us/step - loss: 0.2928 - mean_squared_error: 0.2928\n",
      "Epoch 73/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2926 - mean_squared_error: 0.2926\n",
      "Epoch 74/800\n",
      "299/299 [==============================] - 0s 496us/step - loss: 0.2921 - mean_squared_error: 0.2921\n",
      "Epoch 75/800\n",
      "299/299 [==============================] - 0s 465us/step - loss: 0.2958 - mean_squared_error: 0.2958\n",
      "Epoch 76/800\n",
      "299/299 [==============================] - 0s 838us/step - loss: 0.2919 - mean_squared_error: 0.2919\n",
      "Epoch 77/800\n",
      "299/299 [==============================] - 0s 543us/step - loss: 0.2969 - mean_squared_error: 0.2969\n",
      "Epoch 78/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2949 - mean_squared_error: 0.2949\n",
      "Epoch 79/800\n",
      "299/299 [==============================] - 0s 440us/step - loss: 0.2922 - mean_squared_error: 0.2922\n",
      "Epoch 80/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.2927 - mean_squared_error: 0.2927\n",
      "Epoch 81/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2927 - mean_squared_error: 0.2927\n",
      "Epoch 82/800\n",
      "299/299 [==============================] - 0s 439us/step - loss: 0.2912 - mean_squared_error: 0.2912\n",
      "Epoch 83/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.2921 - mean_squared_error: 0.2921\n",
      "Epoch 84/800\n",
      "299/299 [==============================] - 0s 470us/step - loss: 0.2969 - mean_squared_error: 0.2969\n",
      "Epoch 85/800\n",
      "299/299 [==============================] - 0s 441us/step - loss: 0.2947 - mean_squared_error: 0.2947\n",
      "Epoch 86/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2905 - mean_squared_error: 0.2905\n",
      "Epoch 87/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2928 - mean_squared_error: 0.2928\n",
      "Epoch 88/800\n",
      "299/299 [==============================] - 0s 463us/step - loss: 0.2924 - mean_squared_error: 0.2924\n",
      "Epoch 89/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.2935 - mean_squared_error: 0.2935\n",
      "Epoch 90/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2922 - mean_squared_error: 0.2922\n",
      "Epoch 91/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2907 - mean_squared_error: 0.2907\n",
      "Epoch 92/800\n",
      "299/299 [==============================] - 0s 447us/step - loss: 0.2928 - mean_squared_error: 0.2928\n",
      "Epoch 93/800\n",
      "299/299 [==============================] - 0s 435us/step - loss: 0.2953 - mean_squared_error: 0.2953\n",
      "Epoch 94/800\n",
      "299/299 [==============================] - 0s 438us/step - loss: 0.2900 - mean_squared_error: 0.2900\n",
      "Epoch 95/800\n",
      "299/299 [==============================] - 0s 439us/step - loss: 0.2917 - mean_squared_error: 0.2917\n",
      "Epoch 96/800\n",
      "299/299 [==============================] - 0s 440us/step - loss: 0.2929 - mean_squared_error: 0.2929\n",
      "Epoch 97/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2928 - mean_squared_error: 0.2928\n",
      "Epoch 98/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2935 - mean_squared_error: 0.2935\n",
      "Epoch 99/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2929 - mean_squared_error: 0.2929\n",
      "Epoch 100/800\n",
      "299/299 [==============================] - 0s 471us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 101/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2939 - mean_squared_error: 0.2939\n",
      "Epoch 102/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2901 - mean_squared_error: 0.2901\n",
      "Epoch 103/800\n",
      "299/299 [==============================] - 0s 495us/step - loss: 0.2898 - mean_squared_error: 0.2898\n",
      "Epoch 104/800\n",
      "299/299 [==============================] - 0s 784us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 105/800\n",
      "299/299 [==============================] - 0s 915us/step - loss: 0.2911 - mean_squared_error: 0.2911\n",
      "Epoch 106/800\n",
      "299/299 [==============================] - 0s 601us/step - loss: 0.2919 - mean_squared_error: 0.2919\n",
      "Epoch 107/800\n",
      "299/299 [==============================] - 0s 464us/step - loss: 0.2918 - mean_squared_error: 0.2918\n",
      "Epoch 108/800\n",
      "299/299 [==============================] - 0s 457us/step - loss: 0.2898 - mean_squared_error: 0.2898\n",
      "Epoch 109/800\n",
      "299/299 [==============================] - 0s 459us/step - loss: 0.2902 - mean_squared_error: 0.2902\n",
      "Epoch 110/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2909 - mean_squared_error: 0.2909\n",
      "Epoch 111/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 112/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2925 - mean_squared_error: 0.2925\n",
      "Epoch 113/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2899 - mean_squared_error: 0.2899\n",
      "Epoch 114/800\n",
      "299/299 [==============================] - 0s 900us/step - loss: 0.2888 - mean_squared_error: 0.2888\n",
      "Epoch 115/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2921 - mean_squared_error: 0.2921\n",
      "Epoch 116/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2887 - mean_squared_error: 0.2887\n",
      "Epoch 117/800\n",
      "299/299 [==============================] - 0s 457us/step - loss: 0.2899 - mean_squared_error: 0.2899\n",
      "Epoch 118/800\n",
      "299/299 [==============================] - 0s 457us/step - loss: 0.2923 - mean_squared_error: 0.2923\n",
      "Epoch 119/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2909 - mean_squared_error: 0.2909\n",
      "Epoch 120/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.2898 - mean_squared_error: 0.2898\n",
      "Epoch 121/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2920 - mean_squared_error: 0.2920\n",
      "Epoch 122/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2928 - mean_squared_error: 0.2928\n",
      "Epoch 123/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2903 - mean_squared_error: 0.2903\n",
      "Epoch 124/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.2959 - mean_squared_error: 0.2959\n",
      "Epoch 125/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2915 - mean_squared_error: 0.2915\n",
      "Epoch 126/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2882 - mean_squared_error: 0.2882\n",
      "Epoch 127/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2900 - mean_squared_error: 0.2900\n",
      "Epoch 128/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2930 - mean_squared_error: 0.2930\n",
      "Epoch 129/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.2914 - mean_squared_error: 0.2914\n",
      "Epoch 130/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2923 - mean_squared_error: 0.2923\n",
      "Epoch 131/800\n",
      "299/299 [==============================] - 0s 485us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 132/800\n",
      "299/299 [==============================] - 0s 458us/step - loss: 0.2907 - mean_squared_error: 0.2907\n",
      "Epoch 133/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2939 - mean_squared_error: 0.2939\n",
      "Epoch 134/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2909 - mean_squared_error: 0.2909\n",
      "Epoch 135/800\n",
      "299/299 [==============================] - 0s 470us/step - loss: 0.2926 - mean_squared_error: 0.2926\n",
      "Epoch 136/800\n",
      "299/299 [==============================] - 0s 439us/step - loss: 0.2897 - mean_squared_error: 0.2897\n",
      "Epoch 137/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2881 - mean_squared_error: 0.2881\n",
      "Epoch 138/800\n",
      "299/299 [==============================] - 0s 872us/step - loss: 0.2889 - mean_squared_error: 0.2889\n",
      "Epoch 139/800\n",
      "299/299 [==============================] - 0s 629us/step - loss: 0.2890 - mean_squared_error: 0.2890\n",
      "Epoch 140/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2900 - mean_squared_error: 0.2900\n",
      "Epoch 141/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2903 - mean_squared_error: 0.2903\n",
      "Epoch 142/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2905 - mean_squared_error: 0.2905\n",
      "Epoch 143/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2904 - mean_squared_error: 0.2904\n",
      "Epoch 144/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 145/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2888 - mean_squared_error: 0.2888\n",
      "Epoch 146/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2902 - mean_squared_error: 0.2902\n",
      "Epoch 147/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2881 - mean_squared_error: 0.2881\n",
      "Epoch 148/800\n",
      "299/299 [==============================] - 0s 441us/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 149/800\n",
      "299/299 [==============================] - 0s 897us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 150/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2911 - mean_squared_error: 0.2911\n",
      "Epoch 151/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2880 - mean_squared_error: 0.2880\n",
      "Epoch 152/800\n",
      "299/299 [==============================] - 0s 465us/step - loss: 0.2874 - mean_squared_error: 0.2874\n",
      "Epoch 153/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2889 - mean_squared_error: 0.2889\n",
      "Epoch 154/800\n",
      "299/299 [==============================] - 0s 480us/step - loss: 0.2885 - mean_squared_error: 0.2885\n",
      "Epoch 155/800\n",
      "299/299 [==============================] - 0s 798us/step - loss: 0.2905 - mean_squared_error: 0.2905\n",
      "Epoch 156/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 157/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2905 - mean_squared_error: 0.2905\n",
      "Epoch 158/800\n",
      "299/299 [==============================] - 0s 442us/step - loss: 0.2870 - mean_squared_error: 0.2870\n",
      "Epoch 159/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2907 - mean_squared_error: 0.2907\n",
      "Epoch 160/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2923 - mean_squared_error: 0.2923\n",
      "Epoch 161/800\n",
      "299/299 [==============================] - 0s 459us/step - loss: 0.2875 - mean_squared_error: 0.2875\n",
      "Epoch 162/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.2883 - mean_squared_error: 0.2883\n",
      "Epoch 163/800\n",
      "299/299 [==============================] - 0s 462us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 164/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2879 - mean_squared_error: 0.2879\n",
      "Epoch 165/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 166/800\n",
      "299/299 [==============================] - 0s 451us/step - loss: 0.2935 - mean_squared_error: 0.2935\n",
      "Epoch 167/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2860 - mean_squared_error: 0.2860\n",
      "Epoch 168/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2933 - mean_squared_error: 0.2933\n",
      "Epoch 169/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.2893 - mean_squared_error: 0.2893\n",
      "Epoch 170/800\n",
      "299/299 [==============================] - 0s 447us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 171/800\n",
      "299/299 [==============================] - 0s 854us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 172/800\n",
      "299/299 [==============================] - 0s 587us/step - loss: 0.2867 - mean_squared_error: 0.2867\n",
      "Epoch 173/800\n",
      "299/299 [==============================] - 0s 511us/step - loss: 0.2889 - mean_squared_error: 0.2889\n",
      "Epoch 174/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2910 - mean_squared_error: 0.2910\n",
      "Epoch 175/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2923 - mean_squared_error: 0.2923\n",
      "Epoch 176/800\n",
      "299/299 [==============================] - 0s 440us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 177/800\n",
      "299/299 [==============================] - 0s 455us/step - loss: 0.2877 - mean_squared_error: 0.2877\n",
      "Epoch 178/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.2893 - mean_squared_error: 0.2893\n",
      "Epoch 179/800\n",
      "299/299 [==============================] - 0s 452us/step - loss: 0.2861 - mean_squared_error: 0.2861\n",
      "Epoch 180/800\n",
      "299/299 [==============================] - 0s 888us/step - loss: 0.2894 - mean_squared_error: 0.2894\n",
      "Epoch 181/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2908 - mean_squared_error: 0.2908\n",
      "Epoch 182/800\n",
      "299/299 [==============================] - 0s 460us/step - loss: 0.2885 - mean_squared_error: 0.2885\n",
      "Epoch 183/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2920 - mean_squared_error: 0.2920\n",
      "Epoch 184/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2868 - mean_squared_error: 0.2868\n",
      "Epoch 185/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2908 - mean_squared_error: 0.2908\n",
      "Epoch 186/800\n",
      "299/299 [==============================] - 0s 454us/step - loss: 0.2902 - mean_squared_error: 0.2902\n",
      "Epoch 187/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 188/800\n",
      "299/299 [==============================] - 0s 442us/step - loss: 0.2879 - mean_squared_error: 0.2879\n",
      "Epoch 189/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2883 - mean_squared_error: 0.2883\n",
      "Epoch 190/800\n",
      "299/299 [==============================] - 0s 437us/step - loss: 0.2900 - mean_squared_error: 0.2900\n",
      "Epoch 191/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2884 - mean_squared_error: 0.2884\n",
      "Epoch 192/800\n",
      "299/299 [==============================] - 0s 445us/step - loss: 0.2867 - mean_squared_error: 0.2867\n",
      "Epoch 193/800\n",
      "299/299 [==============================] - 0s 453us/step - loss: 0.2887 - mean_squared_error: 0.2887\n",
      "Epoch 194/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.2881 - mean_squared_error: 0.2881\n",
      "Epoch 195/800\n",
      "299/299 [==============================] - 0s 928us/step - loss: 0.2886 - mean_squared_error: 0.2886\n",
      "Epoch 196/800\n",
      "299/299 [==============================] - 0s 586us/step - loss: 0.2867 - mean_squared_error: 0.2867\n",
      "Epoch 197/800\n",
      "299/299 [==============================] - 0s 444us/step - loss: 0.2875 - mean_squared_error: 0.2875\n",
      "Epoch 198/800\n",
      "299/299 [==============================] - 0s 476us/step - loss: 0.2883 - mean_squared_error: 0.2883\n",
      "Epoch 199/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2892 - mean_squared_error: 0.2892\n",
      "Epoch 200/800\n",
      "299/299 [==============================] - 0s 447us/step - loss: 0.2883 - mean_squared_error: 0.2883\n",
      "Epoch 201/800\n",
      "299/299 [==============================] - 0s 459us/step - loss: 0.2871 - mean_squared_error: 0.2871\n",
      "Epoch 202/800\n",
      "299/299 [==============================] - 0s 449us/step - loss: 0.2867 - mean_squared_error: 0.2867\n",
      "Epoch 203/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2868 - mean_squared_error: 0.2868\n",
      "Epoch 204/800\n",
      "299/299 [==============================] - 0s 447us/step - loss: 0.2893 - mean_squared_error: 0.2893\n",
      "Epoch 205/800\n",
      "299/299 [==============================] - 0s 441us/step - loss: 0.2862 - mean_squared_error: 0.2862\n",
      "Epoch 206/800\n",
      "299/299 [==============================] - 0s 812us/step - loss: 0.2873 - mean_squared_error: 0.2873\n",
      "Epoch 207/800\n",
      "299/299 [==============================] - 0s 521us/step - loss: 0.2904 - mean_squared_error: 0.2904\n",
      "Epoch 208/800\n",
      "299/299 [==============================] - 0s 450us/step - loss: 0.2879 - mean_squared_error: 0.2879\n",
      "Epoch 209/800\n",
      "299/299 [==============================] - 0s 464us/step - loss: 0.2917 - mean_squared_error: 0.2917\n",
      "Epoch 210/800\n",
      "299/299 [==============================] - 0s 648us/step - loss: 0.2903 - mean_squared_error: 0.2903\n",
      "Epoch 211/800\n",
      "299/299 [==============================] - 0s 446us/step - loss: 0.2883 - mean_squared_error: 0.2883\n",
      "Epoch 212/800\n",
      "299/299 [==============================] - 0s 443us/step - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Epoch 213/800\n",
      "299/299 [==============================] - 0s 448us/step - loss: 0.2920 - mean_squared_error: 0.2920\n",
      "Epoch 214/800\n",
      "299/299 [==============================] - 0s 456us/step - loss: 0.2865 - mean_squared_error: 0.2865\n",
      "Epoch 215/800\n",
      "299/299 [==============================] - 0s 656us/step - loss: 0.2870 - mean_squared_error: 0.2870\n",
      "Epoch 216/800\n",
      "299/299 [==============================] - 0s 442us/step - loss: 0.2915 - mean_squared_error: 0.2915\n",
      "Epoch 217/800\n",
      "299/299 [==============================] - 0s 876us/step - loss: 0.2874 - mean_squared_error: 0.2874\n",
      "299/299 [==============================] - 0s 441us/step\n",
      "75/75 [==============================] - 0s 303us/step\n"
     ]
    }
   ],
   "source": [
    "optimal_features = ['T2', 'D8']\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    # Skip features that are already identified as optimal\n",
    "    if feature in optimal_features:\n",
    "        continue\n",
    "\n",
    "    proj_path = f'var{i}'\n",
    "    tune_ANN(df, optimal_features + [feature], proj_path, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal row: F1                0.000000\n",
      "D1                0.000000\n",
      "T1                0.000000\n",
      "P2                0.000000\n",
      "T2                1.000000\n",
      "T3                0.000000\n",
      "T4                1.000000\n",
      "P6                0.000000\n",
      "T7                0.000000\n",
      "L7                0.000000\n",
      "F8                0.000000\n",
      "D8                1.000000\n",
      "T9                0.000000\n",
      "T10               0.000000\n",
      "A11               0.000000\n",
      "B11               0.000000\n",
      "A12               0.000000\n",
      "B12               0.000000\n",
      "Cap               0.000000\n",
      "Training Error    0.062108\n",
      "Test Error        0.051637\n",
      "Name: 46, dtype: float64\n",
      "Optimal row: F1                0.000000\n",
      "D1                0.000000\n",
      "T1                0.000000\n",
      "P2                0.000000\n",
      "T2                1.000000\n",
      "T3                0.000000\n",
      "T4                0.000000\n",
      "P6                0.000000\n",
      "T7                0.000000\n",
      "L7                0.000000\n",
      "F8                0.000000\n",
      "D8                1.000000\n",
      "T9                0.000000\n",
      "T10               0.000000\n",
      "A11               0.000000\n",
      "B11               0.000000\n",
      "A12               0.000000\n",
      "B12               1.000000\n",
      "Cap               0.000000\n",
      "Training Error    0.062364\n",
      "Test Error        0.050184\n",
      "Name: 56, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p2/j2dh2mts4mz6_nskglkx1g9m0000gn/T/ipykernel_93985/1316156385.py:30: UserWarning: Best training and best test are not the same!\n",
      "  warnings.warn(\"Best training and best test are not the same!\")\n",
      "/var/folders/p2/j2dh2mts4mz6_nskglkx1g9m0000gn/T/ipykernel_93985/1316156385.py:30: UserWarning: Best training and best test are not the same!\n",
      "  warnings.warn(\"Best training and best test are not the same!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['T2', 'D8', 'B12']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "def find_optimal_features(df, num_features, method='train'):\n",
    "    \"\"\"\n",
    "    Identifies columns with a value of 1 in the row with the minimum training or test error,\n",
    "    after filtering rows based on the sum of their feature values being equal to num_features.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the data.\n",
    "    - num_features (int): The number of features that must sum to 1 to consider a row.\n",
    "    - method (str): The error type to minimize ('train' or 'test').\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of optimal feature column names.\n",
    "    \"\"\"\n",
    "\n",
    "    df_filtered = df.drop(['Training Error', 'Test Error'], axis=1)\n",
    "    df_filtered['Sum'] = df_filtered.sum(axis=1)\n",
    "    df_features = df_filtered[df_filtered['Sum'] == num_features]\n",
    "    \n",
    "    # Check if there are any rows meeting the criteria\n",
    "    if df_features.empty:\n",
    "        raise ValueError(\"No rows with the specified number of features found.\")\n",
    "    \n",
    "    # Find indexes of rows with minimum error within the filtered DataFrame\n",
    "    idx_train = df_features.join(df['Training Error'])['Training Error'].idxmin()\n",
    "    idx_test = df_features.join(df['Test Error'])['Test Error'].idxmin()\n",
    "    if idx_train != idx_test:\n",
    "        warnings.warn(\"Best training and best test are not the same!\")\n",
    "    \n",
    "    # Select the row based on the specified method\n",
    "    if method == 'train':\n",
    "        optimal_row = df.loc[idx_train]\n",
    "    elif method == 'test':\n",
    "        optimal_row = df.loc[idx_test]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'train' or 'test'\")\n",
    "    \n",
    "    print(f\"Optimal row: {optimal_row}\")\n",
    "    \n",
    "    # Identify columns with a value of 1 in the optimal row\n",
    "    optimal_features = optimal_row.index[optimal_row.eq(1)].tolist()\n",
    "    \n",
    "    return optimal_features\n",
    "\n",
    "sfs_df = pd.read_pickle(file_path)\n",
    "find_optimal_features(sfs_df, 3, 'train')\n",
    "find_optimal_features(sfs_df, 3, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 1 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features = []\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    # Skip features that are already identified as optimal\n",
    "    if feature in optimal_features:\n",
    "        continue\n",
    "\n",
    "    proj_path = f'var{i}'\n",
    "    if feature == 'T2':\n",
    "        model = tune_ANN(df, optimal_features + [feature], proj_path)\n",
    "\n",
    "df_train, rmse_train = prediction_df(model, scaler, X_train[['Time', 'T2']], y_train)\n",
    "df_test, rmse_test = prediction_df(model, scaler, X_test[['Time', 'T2']], y_test)\n",
    "\n",
    "# Parity plot\n",
    "limit = np.array([3.5, 4.6])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.plot(limit, limit, color='black', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 1.01*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 0.99*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.1, 1.01*limit[1]-0.18, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=45)\n",
    "plt.text(limit[1]-0.05, 0.99*limit[1]-0.09, '-1% error', verticalalignment='top', horizontalalignment='right', color='black', rotation=45)\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(limit)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Parity_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Residuals plot\n",
    "mean = df['SRD'].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axhline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axhline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.07, 0.0105*mean, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black')\n",
    "plt.text(limit[1]-0.07, -0.0105*mean, '-1% error', verticalalignment='top', horizontalalignment='right', color='black')\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Residuals_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Residuals histogram plot\n",
    "mean = df['SRD'].mean()\n",
    "x_lim = np.array([-0.2, 0.2])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = plt.gca()\n",
    "yticks = ax.yaxis.get_major_ticks() \n",
    "yticks[0].label1.set_visible(False)\n",
    "\n",
    "sns.histplot(df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9)\n",
    "sns.histplot(df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.8)\n",
    "\n",
    "plt.text(0.011*mean, 500, '+1% error', verticalalignment='bottom', horizontalalignment='left', color='black', rotation=90)\n",
    "plt.text(-0.01*mean, 500, '-1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=90)\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axvline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axvline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5, loc='upper left', shadow=True, fancybox=True)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"ResidualHistogram_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Density plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.kdeplot(df_train['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Actual')\n",
    "sns.kdeplot(df_train['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Predicted')\n",
    "axs[0].legend(loc='upper left', shadow=True, fancybox=True)\n",
    "axs[0].set_xlim(3.5, 4.5)\n",
    "axs[0].set_title('Training')\n",
    "\n",
    "sns.kdeplot(df_test['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Actual')\n",
    "sns.kdeplot(df_test['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Predicted')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlim(3.5, 4.5)\n",
    "axs[1].set_title('Testing')\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"KDE_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features = ['T2']\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    # Skip features that are already identified as optimal\n",
    "    if feature in optimal_features:\n",
    "        continue\n",
    "\n",
    "    proj_path = f'var{i}'\n",
    "    if feature == 'D8':\n",
    "        model = tune_ANN(df, optimal_features + [feature], proj_path)\n",
    "\n",
    "df_train, rmse_train = prediction_df(model, \n",
    "                                     scaler, \n",
    "                                     X_train[['Time']+optimal_features + ['D8']], \n",
    "                                     y_train)\n",
    "df_test, rmse_test = prediction_df(model, \n",
    "                                   scaler,\n",
    "                                   X_test[['Time']+optimal_features + ['D8']], \n",
    "                                   y_test)\n",
    "\n",
    "# Parity plot\n",
    "limit = np.array([3.5, 4.6])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.plot(limit, limit, color='black', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 1.01*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 0.99*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.1, 1.01*limit[1]-0.18, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=45)\n",
    "plt.text(limit[1]-0.05, 0.99*limit[1]-0.09, '-1% error', verticalalignment='top', horizontalalignment='right', color='black', rotation=45)\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(limit)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Parity_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Residuals plot\n",
    "mean = df['SRD'].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axhline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axhline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.07, 0.0105*mean, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black')\n",
    "plt.text(limit[1]-0.07, -0.0105*mean, '-1% error', verticalalignment='top', horizontalalignment='right', color='black')\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Residuals_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Residuals histogram plot\n",
    "mean = df['SRD'].mean()\n",
    "x_lim = np.array([-0.2, 0.2])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = plt.gca()\n",
    "yticks = ax.yaxis.get_major_ticks() \n",
    "yticks[0].label1.set_visible(False)\n",
    "\n",
    "sns.histplot(df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9)\n",
    "sns.histplot(df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.8)\n",
    "\n",
    "plt.text(0.011*mean, 500, '+1% error', verticalalignment='bottom', horizontalalignment='left', color='black', rotation=90)\n",
    "plt.text(-0.01*mean, 500, '-1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=90)\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axvline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axvline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5, loc='upper left', shadow=True, fancybox=True)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"ResidualHistogram_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Density plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.kdeplot(df_train['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Actual')\n",
    "sns.kdeplot(df_train['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Predicted')\n",
    "axs[0].legend(loc='upper left', shadow=True, fancybox=True)\n",
    "axs[0].set_xlim(3.5, 4.5)\n",
    "axs[0].set_title('Training')\n",
    "\n",
    "sns.kdeplot(df_test['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Actual')\n",
    "sns.kdeplot(df_test['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Predicted')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlim(3.5, 4.5)\n",
    "axs[1].set_title('Testing')\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"KDE_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features = ['T2', 'D8']\n",
    "for i, feature in enumerate(features, start=1):\n",
    "    # Skip features that are already identified as optimal\n",
    "    if feature in optimal_features:\n",
    "        continue\n",
    "\n",
    "    proj_path = f'var{i}'\n",
    "    if feature == 'B12':\n",
    "        model = tune_ANN(df, optimal_features + [feature], proj_path)\n",
    "\n",
    "df_train, rmse_train = prediction_df(model, \n",
    "                                     scaler, \n",
    "                                     X_train[['Time']+optimal_features + ['D8']], \n",
    "                                     y_train)\n",
    "df_test, rmse_test = prediction_df(model, \n",
    "                                   scaler,\n",
    "                                   X_test[['Time']+optimal_features + ['D8']], \n",
    "                                   y_test)\n",
    "\n",
    "# Parity plot\n",
    "limit = np.array([3.5, 4.6])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.plot(limit, limit, color='black', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 1.01*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.plot(limit, 0.99*limit, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.1, 1.01*limit[1]-0.18, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=45)\n",
    "plt.text(limit[1]-0.05, 0.99*limit[1]-0.09, '-1% error', verticalalignment='top', horizontalalignment='right', color='black', rotation=45)\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(limit)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Parity_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Residuals plot\n",
    "mean = df['SRD'].mean()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x=df_train['Actual'], y=df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9, s=50)\n",
    "sns.scatterplot(x=df_test['Actual'], y=df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.9, s=50)\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axhline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axhline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.text(limit[1]-0.07, 0.0105*mean, '+1% error', verticalalignment='bottom', horizontalalignment='right', color='black')\n",
    "plt.text(limit[1]-0.07, -0.0105*mean, '-1% error', verticalalignment='top', horizontalalignment='right', color='black')\n",
    "\n",
    "plt.xlim(limit)\n",
    "plt.ylim(-0.3, 0.3)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"Residuals_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Residuals histogram plot\n",
    "mean = df['SRD'].mean()\n",
    "x_lim = np.array([-0.2, 0.2])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = plt.gca()\n",
    "yticks = ax.yaxis.get_major_ticks() \n",
    "yticks[0].label1.set_visible(False)\n",
    "\n",
    "sns.histplot(df_train['Actual'] - df_train['Predicted'], label='Train', alpha=0.9)\n",
    "sns.histplot(df_test['Actual'] - df_test['Predicted'], label='Test', alpha=0.8)\n",
    "\n",
    "plt.text(0.011*mean, 500, '+1% error', verticalalignment='bottom', horizontalalignment='left', color='black', rotation=90)\n",
    "plt.text(-0.01*mean, 500, '-1% error', verticalalignment='bottom', horizontalalignment='right', color='black', rotation=90)\n",
    "\n",
    "plt.axvline(0, color='black', linestyle='-', alpha=0.7, linewidth=2)\n",
    "plt.axvline(0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "plt.axvline(-0.01*mean, color='black', linestyle='--', alpha=0.7, linewidth=2)\n",
    "\n",
    "plt.xlim(x_lim)\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Residual')\n",
    "plt.legend(markerscale=1.5, loc='upper left', shadow=True, fancybox=True)\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"ResidualHistogram_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()\n",
    "\n",
    "# Density plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.kdeplot(df_train['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Actual')\n",
    "sns.kdeplot(df_train['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[0], label='Predicted')\n",
    "axs[0].legend(loc='upper left', shadow=True, fancybox=True)\n",
    "axs[0].set_xlim(3.5, 4.5)\n",
    "axs[0].set_title('Training')\n",
    "\n",
    "sns.kdeplot(df_test['Actual'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Actual')\n",
    "sns.kdeplot(df_test['Predicted'], linewidth=2, alpha=0.7, fill=True, ax=axs[1], label='Predicted')\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[1].set_xlim(3.5, 4.5)\n",
    "axs[1].set_title('Testing')\n",
    "\n",
    "#utils.save_file(plt.gcf(), plots_path, \"KDE_plot_ANN.pdf\", 'matplotlib', overwrite)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XingPythonKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
