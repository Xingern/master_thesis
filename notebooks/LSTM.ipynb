{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read & manipulate data\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random as rn\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "RANDOM_SEED = 420\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU since slow on my machine\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "pkl_path = '/Users/junxingli/Desktop/master_thesis/data/processed/SRD_Lysekil.pkl'\n",
    "tuner_path = '/Users/junxingli/Desktop/master_thesis/models/lstm/'\n",
    "proj_name = 'whole-V3/'\n",
    "best_model_path = tuner_path + proj_name + 'best_model'\n",
    "\n",
    "# Figure paths\n",
    "overwrite = False\n",
    "figs_path = '/Users/junxingli/Desktop/master_thesis/figs/lstm/'\n",
    "plots_path = figs_path + proj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps):\n",
    "    ts = X['Time'].values[time_steps:]\n",
    "    X = X.drop('Time', axis=1).values\n",
    "    y = y.drop('Time', axis=1).values\n",
    "\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(np.hstack([X[i:(i+time_steps)], y[i:(i+time_steps)]]))\n",
    "        ys.append(y[i+time_steps])\n",
    "\n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1), np.array(ts)\n",
    "\n",
    "def run_preprocessing(case='whole_data'):\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    if case == \"only_stable\":\n",
    "        df = df[df['Status'] == 'Stable']\n",
    "\n",
    "    X = df.drop(['Status', 'SRD'], axis=1)\n",
    "    y = df[['Time', 'SRD']]\n",
    "\n",
    "    cutoff_date = \"2020-06-15 00:00\"\n",
    "    X_train = X[X[\"Time\"] < cutoff_date].copy()\n",
    "    X_test = X[X[\"Time\"] > cutoff_date].copy()\n",
    "    y_train = y[y[\"Time\"] < cutoff_date].copy()\n",
    "    y_test = y[y[\"Time\"] > cutoff_date].copy()\n",
    "\n",
    "    # Standard scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    cols = [col for col in X_train.columns if col != 'Time']\n",
    "    X_train[cols] = scaler.fit_transform(X_train[cols])\n",
    "    X_test[cols] = scaler.transform(X_test[cols])\n",
    "\n",
    "    # Need to reshape since the scaler expects a 2D array\n",
    "    y_train['SRD'] = scaler.fit_transform(y_train['SRD'].to_numpy().reshape(-1, 1))\n",
    "    y_test['SRD'] = scaler.transform(y_test['SRD'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "# Create sequences for LSTM\n",
    "time_steps = 15\n",
    "X_train, y_train, X_test, y_test, scaler = run_preprocessing(case='whole_data')\n",
    "X_train_seq, y_train_seq, times_train = create_sequences(X_train, y_train, time_steps)\n",
    "X_test_seq, y_test_seq, times_test = create_sequences(X_test, y_test, time_steps)\n",
    "\n",
    "input_dim = X_train_seq.shape[-1]\n",
    "output_dim = y_train_seq.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        keras.backend.clear_session()\n",
    "        # Hyperparameters\n",
    "        hp_layers = hp.Int('num_layers', 1, 3) \n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])\n",
    "        hp_optimizer = hp.Choice('optimizer', values=['adam']) #'SGD', 'rmsprop'])\n",
    "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(time_steps, input_dim)))\n",
    "        # Only the last LSTM layer has return_sequences=False\n",
    "        for i in range(hp_layers):\n",
    "            if i < hp_layers - 1:\n",
    "                return_sequences = True\n",
    "            else:\n",
    "                return_sequences = False\n",
    "            \n",
    "            hp_units = hp.Int('num_units_' + str(i), min_value=60, max_value=200, step=10)\n",
    "            hp_dropout = hp.Float('dropout_rate_' + str(i), min_value=0, max_value=0.40, step=0.1)\n",
    "            \n",
    "            model.add(LSTM(units=hp_units, \n",
    "                           activation=hp_activation, \n",
    "                           return_sequences=return_sequences))\n",
    "            if hp_dropout > 0:\n",
    "                model.add(Dropout(rate=hp_dropout))\n",
    "\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Select optimizer  \n",
    "        optimizer_dict = {\n",
    "            'adam': keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "            'SGD': keras.optimizers.legacy.SGD(learning_rate=hp_learning_rate),\n",
    "            'rmsprop': keras.optimizers.legacy.RMSprop(learning_rate=hp_learning_rate)\n",
    "        }\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer_dict[hp_optimizer],\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error', \n",
    "                     'mean_absolute_error', \n",
    "                     'mean_absolute_percentage_error']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(MyHyperModel(),\n",
    "                     objective='val_mean_squared_error',\n",
    "                     max_epochs=100,\n",
    "                     overwrite=False,\n",
    "                     directory=tuner_path,\n",
    "                     project_name=proj_name)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', \n",
    "                              factor=0.2,\n",
    "                              patience=5, \n",
    "                              min_lr=0.001)\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_mean_squared_error', \n",
    "                           patience=10)\n",
    "\n",
    "tuner.search(X_train_seq,\n",
    "             y_train_seq, \n",
    "             epochs=100, \n",
    "             validation_split=0.25, \n",
    "             callbacks=[stop_early, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        x, y = self.test_data\n",
    "        results = self.model.evaluate(x, y, verbose=0)\n",
    "        logs['test_mse'] = results if isinstance(results, float) else results[1]\n",
    "        print('\\nTest MSE:', logs['test_mse'])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='mean_squared_error', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "\n",
    "stop_early = EarlyStopping(monitor='mean_squared_error', \n",
    "                                              patience=20)\n",
    "\n",
    "test_callback = TestCallback((X_test_seq, y_test_seq))\n",
    "\n",
    "best_model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error'])\n",
    "\n",
    "History = best_model.fit(X_train_seq, \n",
    "                         y_train_seq, \n",
    "                         epochs=5, \n",
    "                         callbacks=[stop_early, reduce_lr, test_callback],\n",
    "                         validation_split=0)\n",
    "\n",
    "best_model.summary()\n",
    "print(f\"\"\"{best_hps.values}\n",
    "-------------------------------------\n",
    "Model saved to {best_model_path}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite or not os.path.exists(best_model_path):\n",
    "    print(f\"Model was saved to {best_model_path}\")\n",
    "    best_model.save(best_model_path)\n",
    "else:\n",
    "    print(f\"Model file already exists at {best_model_path}. Set 'overwrite=True' to overwrite the file.\")\n",
    "    \n",
    "utils.plot_mse_over_epochs(History, plots_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_df(model, scaler, X_seq, y_seq, times):\n",
    "    y_pred = model.predict(X_seq)\n",
    "    y_pred = scaler.inverse_transform(y_pred).reshape(-1)\n",
    "    y_actual = scaler.inverse_transform(y_seq).reshape(-1)\n",
    "\n",
    "    df_res = pd.DataFrame({'Time': times, \n",
    "                           'Actual': y_actual, \n",
    "                           'Predicted': y_pred})\n",
    "    df_res.sort_values('Time')\n",
    "    \n",
    "    rmse = mean_squared_error(df_res['Actual'], df_res['Predicted'], squared=False)\n",
    "    r2 = r2_score(df_res['Actual'], df_res['Predicted'])\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    \n",
    "    return df_res, rmse\n",
    "\n",
    "loaded_model = keras.models.load_model(best_model_path)\n",
    "df_train, rmse_train = prediction_df(loaded_model, scaler, X_train_seq, y_train_seq, times_train)\n",
    "df_test, rmse_test = prediction_df(loaded_model, scaler, X_test_seq, y_test_seq, times_test)\n",
    "\n",
    "utils.plot_time_predictions(df_train, df_test, plots_path, overwrite=True, limit=[3.5, 4.4], linewidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.make_evaluation_plots(df_train,\n",
    "                            df_test,\n",
    "                            'LSTM-whole-data',\n",
    "                            plots_path,\n",
    "                            overwrite=False,\n",
    "                            limit=[3.4, 5],\n",
    "                            error_line=0.05,\n",
    "                            res_limit=[-0.8, 0.6],\n",
    "                            mean=pd.read_pickle(\"../data/processed/SRD_Lysekil.pkl\")['SRD'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XingsKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
