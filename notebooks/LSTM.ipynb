{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read & manipulate data\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random as rn\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "RANDOM_SEED = 420\n",
    "np.random.seed(RANDOM_SEED)\n",
    "rn.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "tf.config.set_visible_devices([], 'GPU') # Disable GPU since slow on my machine\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "pkl_path = '/Users/junxingli/Desktop/master_thesis/data/processed/SRD_Lysekil.pkl'\n",
    "tuner_path = '/Users/junxingli/Desktop/master_thesis/models/lstm/'\n",
    "proj_name = 'whole-V4/'\n",
    "best_model_path = tuner_path + proj_name + 'best_model'\n",
    "\n",
    "# Figure paths\n",
    "overwrite = False\n",
    "figs_path = '/Users/junxingli/Desktop/master_thesis/figs/lstm/'\n",
    "plots_path = figs_path + proj_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps):\n",
    "    ts = X['Time'].values[time_steps:]\n",
    "    X = X.drop('Time', axis=1).values\n",
    "    y = y.drop('Time', axis=1).values\n",
    "\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(np.hstack([X[i:(i+time_steps)], y[i:(i+time_steps)]]))\n",
    "        ys.append(y[i+time_steps])\n",
    "\n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1), np.array(ts)\n",
    "\n",
    "def run_preprocessing(case='whole_data'):\n",
    "    df = pd.read_pickle(pkl_path)\n",
    "    \n",
    "    if case == \"only_stable\":\n",
    "        df = df[df['Status'] == 'Stable']\n",
    "\n",
    "    X = df.drop(['Status', 'SRD'], axis=1)\n",
    "    y = df[['Time', 'SRD']]\n",
    "\n",
    "    cutoff_date = \"2020-06-15 00:00\"\n",
    "    X_train = X[X[\"Time\"] < cutoff_date].copy()\n",
    "    X_test = X[X[\"Time\"] > cutoff_date].copy()\n",
    "    y_train = y[y[\"Time\"] < cutoff_date].copy()\n",
    "    y_test = y[y[\"Time\"] > cutoff_date].copy()\n",
    "\n",
    "    # Standard scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    cols = [col for col in X_train.columns if col != 'Time']\n",
    "    X_train[cols] = scaler.fit_transform(X_train[cols])\n",
    "    X_test[cols] = scaler.transform(X_test[cols])\n",
    "\n",
    "    # Need to reshape since the scaler expects a 2D array\n",
    "    y_train['SRD'] = scaler.fit_transform(y_train['SRD'].to_numpy().reshape(-1, 1))\n",
    "    y_test['SRD'] = scaler.transform(y_test['SRD'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "# Create sequences for LSTM\n",
    "time_steps = 15\n",
    "X_train, y_train, X_test, y_test, scaler = run_preprocessing(case='whole_data')\n",
    "X_train_seq, y_train_seq, times_train = create_sequences(X_train, y_train, time_steps)\n",
    "X_test_seq, y_test_seq, times_test = create_sequences(X_test, y_test, time_steps)\n",
    "\n",
    "input_dim = X_train_seq.shape[-1]\n",
    "output_dim = y_train_seq.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        keras.backend.clear_session()\n",
    "        # Hyperparameters\n",
    "        hp_layers = hp.Int('num_layers', 1, 1) \n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])\n",
    "        hp_optimizer = hp.Choice('optimizer', values=['adam'])#, 'SGD', 'rmsprop'])\n",
    "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(time_steps, input_dim)))\n",
    "        # Only the last LSTM layer has return_sequences=False\n",
    "        for i in range(hp_layers):\n",
    "            if i < hp_layers - 1:\n",
    "                return_sequences = True\n",
    "            else:\n",
    "                return_sequences = False\n",
    "            \n",
    "            hp_units = hp.Int('num_units_' + str(i), min_value=10, max_value=100, step=5)\n",
    "            hp_dropout = hp.Float('dropout_rate_' + str(i), min_value=0, max_value=0.50, step=0.1)\n",
    "            \n",
    "            model.add(LSTM(units=hp_units, \n",
    "                           activation=hp_activation, \n",
    "                           return_sequences=return_sequences))\n",
    "            if hp_dropout > 0:\n",
    "                model.add(Dropout(rate=hp_dropout))\n",
    "\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        # Select optimizer  \n",
    "        optimizer_dict = {\n",
    "            'adam': keras.optimizers.legacy.Adam(learning_rate=hp_learning_rate),\n",
    "            'SGD': keras.optimizers.legacy.SGD(learning_rate=hp_learning_rate),\n",
    "            'rmsprop': keras.optimizers.legacy.RMSprop(learning_rate=hp_learning_rate)\n",
    "        }\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer_dict[hp_optimizer],\n",
    "            loss='mean_squared_error',\n",
    "            metrics=['mean_squared_error', \n",
    "                     'mean_absolute_error', \n",
    "                     'mean_absolute_percentage_error']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [32, 64, 128]),\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 254 Complete [00h 00m 20s]\n",
      "val_mean_squared_error: 0.9801685214042664\n",
      "\n",
      "Best val_mean_squared_error So Far: 0.9604975581169128\n",
      "Total elapsed time: 00h 43m 42s\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(MyHyperModel(),\n",
    "                     objective='val_mean_squared_error',\n",
    "                     max_epochs=100,\n",
    "                     overwrite=False,\n",
    "                     directory=tuner_path,\n",
    "                     project_name=proj_name)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', \n",
    "                              factor=0.2,\n",
    "                              patience=5, \n",
    "                              min_lr=0.001)\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_mean_squared_error', \n",
    "                           patience=10)\n",
    "\n",
    "tuner.search(X_train_seq,\n",
    "             y_train_seq, \n",
    "             epochs=100, \n",
    "             validation_split=0.25, \n",
    "             callbacks=[stop_early, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "426/441 [===========================>..] - ETA: 0s - loss: 0.3615 - mean_squared_error: 0.3615\n",
      "Test MSE: 0.24651171267032623\n",
      "441/441 [==============================] - 2s 3ms/step - loss: 0.3504 - mean_squared_error: 0.3504 - val_loss: 0.9985 - val_mean_squared_error: 0.9985 - lr: 0.0010 - test_mse: 0.2465\n",
      "Epoch 2/100\n",
      "432/441 [============================>.] - ETA: 0s - loss: 0.3136 - mean_squared_error: 0.3136\n",
      "Test MSE: 0.312750905752182\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.3086 - mean_squared_error: 0.3086 - val_loss: 1.0299 - val_mean_squared_error: 1.0299 - lr: 0.0010 - test_mse: 0.3128\n",
      "Epoch 3/100\n",
      "424/441 [===========================>..] - ETA: 0s - loss: 0.3158 - mean_squared_error: 0.3158\n",
      "Test MSE: 0.31377699971199036\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.3047 - mean_squared_error: 0.3047 - val_loss: 1.0328 - val_mean_squared_error: 1.0328 - lr: 0.0010 - test_mse: 0.3138\n",
      "Epoch 4/100\n",
      "430/441 [============================>.] - ETA: 0s - loss: 0.3028 - mean_squared_error: 0.3028\n",
      "Test MSE: 0.32664430141448975\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2960 - mean_squared_error: 0.2960 - val_loss: 1.0086 - val_mean_squared_error: 1.0086 - lr: 0.0010 - test_mse: 0.3266\n",
      "Epoch 5/100\n",
      "423/441 [===========================>..] - ETA: 0s - loss: 0.2827 - mean_squared_error: 0.2827\n",
      "Test MSE: 0.19637255370616913\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2718 - mean_squared_error: 0.2718 - val_loss: 0.9909 - val_mean_squared_error: 0.9909 - lr: 0.0010 - test_mse: 0.1964\n",
      "Epoch 6/100\n",
      "425/441 [===========================>..] - ETA: 0s - loss: 0.3579 - mean_squared_error: 0.3579\n",
      "Test MSE: 0.48688095808029175\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.3458 - mean_squared_error: 0.3458 - val_loss: 1.0102 - val_mean_squared_error: 1.0102 - lr: 0.0010 - test_mse: 0.4869\n",
      "Epoch 7/100\n",
      "437/441 [============================>.] - ETA: 0s - loss: 0.2747 - mean_squared_error: 0.2747\n",
      "Test MSE: 0.2866816818714142\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2728 - mean_squared_error: 0.2728 - val_loss: 1.0027 - val_mean_squared_error: 1.0027 - lr: 0.0010 - test_mse: 0.2867\n",
      "Epoch 8/100\n",
      "436/441 [============================>.] - ETA: 0s - loss: 0.3030 - mean_squared_error: 0.3030\n",
      "Test MSE: 0.2285766452550888\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.3001 - mean_squared_error: 0.3001 - val_loss: 1.1988 - val_mean_squared_error: 1.1988 - lr: 0.0010 - test_mse: 0.2286\n",
      "Epoch 9/100\n",
      "436/441 [============================>.] - ETA: 0s - loss: 0.3108 - mean_squared_error: 0.3108\n",
      "Test MSE: 0.2561516761779785\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.3134 - mean_squared_error: 0.3134 - val_loss: 0.9781 - val_mean_squared_error: 0.9781 - lr: 0.0010 - test_mse: 0.2562\n",
      "Epoch 10/100\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2848 - mean_squared_error: 0.2848\n",
      "Test MSE: 0.27819693088531494\n",
      "441/441 [==============================] - 2s 4ms/step - loss: 0.2848 - mean_squared_error: 0.2848 - val_loss: 1.0095 - val_mean_squared_error: 1.0095 - lr: 0.0010 - test_mse: 0.2782\n",
      "Epoch 11/100\n",
      "425/441 [===========================>..] - ETA: 0s - loss: 0.2801 - mean_squared_error: 0.2801\n",
      "Test MSE: 0.2389516979455948\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2715 - mean_squared_error: 0.2715 - val_loss: 0.9877 - val_mean_squared_error: 0.9877 - lr: 0.0010 - test_mse: 0.2390\n",
      "Epoch 12/100\n",
      "425/441 [===========================>..] - ETA: 0s - loss: 0.2879 - mean_squared_error: 0.2879\n",
      "Test MSE: 0.25787755846977234\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2781 - mean_squared_error: 0.2781 - val_loss: 0.9872 - val_mean_squared_error: 0.9872 - lr: 0.0010 - test_mse: 0.2579\n",
      "Epoch 13/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2854 - mean_squared_error: 0.2854\n",
      "Test MSE: 0.2730894088745117\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2809 - mean_squared_error: 0.2809 - val_loss: 0.9851 - val_mean_squared_error: 0.9851 - lr: 0.0010 - test_mse: 0.2731\n",
      "Epoch 14/100\n",
      "428/441 [============================>.] - ETA: 0s - loss: 0.2756 - mean_squared_error: 0.2756\n",
      "Test MSE: 0.6838369965553284\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2681 - mean_squared_error: 0.2681 - val_loss: 1.0279 - val_mean_squared_error: 1.0279 - lr: 0.0010 - test_mse: 0.6838\n",
      "Epoch 15/100\n",
      "434/441 [============================>.] - ETA: 0s - loss: 0.2700 - mean_squared_error: 0.2700\n",
      "Test MSE: 0.9507380723953247\n",
      "441/441 [==============================] - 2s 4ms/step - loss: 0.2662 - mean_squared_error: 0.2662 - val_loss: 1.0557 - val_mean_squared_error: 1.0557 - lr: 0.0010 - test_mse: 0.9507\n",
      "Epoch 16/100\n",
      "437/441 [============================>.] - ETA: 0s - loss: 0.2730 - mean_squared_error: 0.2730\n",
      "Test MSE: 0.7114257216453552\n",
      "441/441 [==============================] - 2s 4ms/step - loss: 0.2710 - mean_squared_error: 0.2710 - val_loss: 1.0297 - val_mean_squared_error: 1.0297 - lr: 0.0010 - test_mse: 0.7114\n",
      "Epoch 17/100\n",
      "431/441 [============================>.] - ETA: 0s - loss: 0.2779 - mean_squared_error: 0.2779\n",
      "Test MSE: 0.6715524792671204\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2740 - mean_squared_error: 0.2740 - val_loss: 1.0477 - val_mean_squared_error: 1.0477 - lr: 0.0010 - test_mse: 0.6716\n",
      "Epoch 18/100\n",
      "435/441 [============================>.] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.2877\n",
      "Test MSE: 0.7930545806884766\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2843 - mean_squared_error: 0.2843 - val_loss: 1.0430 - val_mean_squared_error: 1.0430 - lr: 0.0010 - test_mse: 0.7931\n",
      "Epoch 19/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2849 - mean_squared_error: 0.2849\n",
      "Test MSE: 0.7269212007522583\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2803 - mean_squared_error: 0.2803 - val_loss: 1.0447 - val_mean_squared_error: 1.0447 - lr: 0.0010 - test_mse: 0.7269\n",
      "Epoch 20/100\n",
      "434/441 [============================>.] - ETA: 0s - loss: 0.2948 - mean_squared_error: 0.2948\n",
      "Test MSE: 0.7867013216018677\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2906 - mean_squared_error: 0.2906 - val_loss: 1.0489 - val_mean_squared_error: 1.0489 - lr: 0.0010 - test_mse: 0.7867\n",
      "Epoch 21/100\n",
      "438/441 [============================>.] - ETA: 0s - loss: 0.2760 - mean_squared_error: 0.2760\n",
      "Test MSE: 0.9966692328453064\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2746 - mean_squared_error: 0.2746 - val_loss: 1.0878 - val_mean_squared_error: 1.0878 - lr: 0.0010 - test_mse: 0.9967\n",
      "Epoch 22/100\n",
      "435/441 [============================>.] - ETA: 0s - loss: 0.1711 - mean_squared_error: 0.1711\n",
      "Test MSE: 1.0306131839752197\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2750 - mean_squared_error: 0.2750 - val_loss: 1.0673 - val_mean_squared_error: 1.0673 - lr: 0.0010 - test_mse: 1.0306\n",
      "Epoch 23/100\n",
      "429/441 [============================>.] - ETA: 0s - loss: 0.2821 - mean_squared_error: 0.2821\n",
      "Test MSE: 1.3275556564331055\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2751 - mean_squared_error: 0.2751 - val_loss: 1.0815 - val_mean_squared_error: 1.0815 - lr: 0.0010 - test_mse: 1.3276\n",
      "Epoch 24/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2968 - mean_squared_error: 0.2968\n",
      "Test MSE: 1.1862975358963013\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2920 - mean_squared_error: 0.2920 - val_loss: 1.0490 - val_mean_squared_error: 1.0490 - lr: 0.0010 - test_mse: 1.1863\n",
      "Epoch 25/100\n",
      "426/441 [===========================>..] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.2776\n",
      "Test MSE: 0.8426310420036316\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2688 - mean_squared_error: 0.2688 - val_loss: 1.0770 - val_mean_squared_error: 1.0770 - lr: 0.0010 - test_mse: 0.8426\n",
      "Epoch 26/100\n",
      "422/441 [===========================>..] - ETA: 0s - loss: 0.2915 - mean_squared_error: 0.2915\n",
      "Test MSE: 0.5911313891410828\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2799 - mean_squared_error: 0.2799 - val_loss: 1.0375 - val_mean_squared_error: 1.0375 - lr: 0.0010 - test_mse: 0.5911\n",
      "Epoch 27/100\n",
      "439/441 [============================>.] - ETA: 0s - loss: 0.2779 - mean_squared_error: 0.2779\n",
      "Test MSE: 0.8885586261749268\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2772 - mean_squared_error: 0.2772 - val_loss: 1.0644 - val_mean_squared_error: 1.0644 - lr: 0.0010 - test_mse: 0.8886\n",
      "Epoch 28/100\n",
      "439/441 [============================>.] - ETA: 0s - loss: 0.2627 - mean_squared_error: 0.2627\n",
      "Test MSE: 0.6943061351776123\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2621 - mean_squared_error: 0.2621 - val_loss: 1.0567 - val_mean_squared_error: 1.0567 - lr: 0.0010 - test_mse: 0.6943\n",
      "Epoch 29/100\n",
      "424/441 [===========================>..] - ETA: 0s - loss: 0.2896 - mean_squared_error: 0.2896\n",
      "Test MSE: 0.7117730975151062\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2792 - mean_squared_error: 0.2792 - val_loss: 1.0921 - val_mean_squared_error: 1.0921 - lr: 0.0010 - test_mse: 0.7118\n",
      "Epoch 30/100\n",
      "439/441 [============================>.] - ETA: 0s - loss: 0.2786 - mean_squared_error: 0.2786\n",
      "Test MSE: 0.6932668089866638\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2778 - mean_squared_error: 0.2778 - val_loss: 1.0753 - val_mean_squared_error: 1.0753 - lr: 0.0010 - test_mse: 0.6933\n",
      "Epoch 31/100\n",
      "438/441 [============================>.] - ETA: 0s - loss: 0.2574 - mean_squared_error: 0.2574\n",
      "Test MSE: 1.856978416442871\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2566 - mean_squared_error: 0.2566 - val_loss: 1.1518 - val_mean_squared_error: 1.1518 - lr: 0.0010 - test_mse: 1.8570\n",
      "Epoch 32/100\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2915 - mean_squared_error: 0.2915\n",
      "Test MSE: 0.6853345632553101\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2915 - mean_squared_error: 0.2915 - val_loss: 1.0369 - val_mean_squared_error: 1.0369 - lr: 0.0010 - test_mse: 0.6853\n",
      "Epoch 33/100\n",
      "422/441 [===========================>..] - ETA: 0s - loss: 0.2799 - mean_squared_error: 0.2799\n",
      "Test MSE: 0.7378461956977844\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2702 - mean_squared_error: 0.2702 - val_loss: 1.0443 - val_mean_squared_error: 1.0443 - lr: 0.0010 - test_mse: 0.7378\n",
      "Epoch 34/100\n",
      "428/441 [============================>.] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.2772\n",
      "Test MSE: 0.5013542771339417\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2702 - mean_squared_error: 0.2702 - val_loss: 1.0196 - val_mean_squared_error: 1.0196 - lr: 0.0010 - test_mse: 0.5014\n",
      "Epoch 35/100\n",
      "439/441 [============================>.] - ETA: 0s - loss: 0.2633 - mean_squared_error: 0.2633\n",
      "Test MSE: 0.34951186180114746\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2625 - mean_squared_error: 0.2625 - val_loss: 1.0067 - val_mean_squared_error: 1.0067 - lr: 0.0010 - test_mse: 0.3495\n",
      "Epoch 36/100\n",
      "440/441 [============================>.] - ETA: 0s - loss: 0.2805 - mean_squared_error: 0.2805\n",
      "Test MSE: 0.3370846211910248\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2804 - mean_squared_error: 0.2804 - val_loss: 1.0350 - val_mean_squared_error: 1.0350 - lr: 0.0010 - test_mse: 0.3371\n",
      "Epoch 37/100\n",
      "428/441 [============================>.] - ETA: 0s - loss: 0.2911 - mean_squared_error: 0.2911\n",
      "Test MSE: 0.3621218800544739\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2835 - mean_squared_error: 0.2835 - val_loss: 1.0246 - val_mean_squared_error: 1.0246 - lr: 0.0010 - test_mse: 0.3621\n",
      "Epoch 38/100\n",
      "427/441 [============================>.] - ETA: 0s - loss: 0.2805 - mean_squared_error: 0.2805\n",
      "Test MSE: 0.460544615983963\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2741 - mean_squared_error: 0.2741 - val_loss: 1.0304 - val_mean_squared_error: 1.0304 - lr: 0.0010 - test_mse: 0.4605\n",
      "Epoch 39/100\n",
      "432/441 [============================>.] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.2679\n",
      "Test MSE: 0.34408077597618103\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2630 - mean_squared_error: 0.2630 - val_loss: 1.0259 - val_mean_squared_error: 1.0259 - lr: 0.0010 - test_mse: 0.3441\n",
      "Epoch 40/100\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2744 - mean_squared_error: 0.2744\n",
      "Test MSE: 0.36515381932258606\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2744 - mean_squared_error: 0.2744 - val_loss: 1.0385 - val_mean_squared_error: 1.0385 - lr: 0.0010 - test_mse: 0.3652\n",
      "Epoch 41/100\n",
      "424/441 [===========================>..] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.2877\n",
      "Test MSE: 0.4472163915634155\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2773 - mean_squared_error: 0.2773 - val_loss: 1.0463 - val_mean_squared_error: 1.0463 - lr: 0.0010 - test_mse: 0.4472\n",
      "Epoch 42/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2706 - mean_squared_error: 0.2706\n",
      "Test MSE: 0.7269973754882812\n",
      "441/441 [==============================] - 2s 5ms/step - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 1.1408 - val_mean_squared_error: 1.1408 - lr: 0.0010 - test_mse: 0.7270\n",
      "Epoch 43/100\n",
      "425/441 [===========================>..] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.2762\n",
      "Test MSE: 0.5482046604156494\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2669 - mean_squared_error: 0.2669 - val_loss: 1.0904 - val_mean_squared_error: 1.0904 - lr: 0.0010 - test_mse: 0.5482\n",
      "Epoch 44/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2807 - mean_squared_error: 0.2807\n",
      "Test MSE: 0.7416084408760071\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2761 - mean_squared_error: 0.2761 - val_loss: 1.1122 - val_mean_squared_error: 1.1122 - lr: 0.0010 - test_mse: 0.7416\n",
      "Epoch 45/100\n",
      "422/441 [===========================>..] - ETA: 0s - loss: 0.2737 - mean_squared_error: 0.2737\n",
      "Test MSE: 0.7938947677612305\n",
      "441/441 [==============================] - 2s 4ms/step - loss: 0.2628 - mean_squared_error: 0.2628 - val_loss: 1.1065 - val_mean_squared_error: 1.1065 - lr: 0.0010 - test_mse: 0.7939\n",
      "Epoch 46/100\n",
      "432/441 [============================>.] - ETA: 0s - loss: 0.2731 - mean_squared_error: 0.2731\n",
      "Test MSE: 1.6029942035675049\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2684 - mean_squared_error: 0.2684 - val_loss: 1.2344 - val_mean_squared_error: 1.2344 - lr: 0.0010 - test_mse: 1.6030\n",
      "Epoch 47/100\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2727 - mean_squared_error: 0.2727\n",
      "Test MSE: 1.1996885538101196\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2727 - mean_squared_error: 0.2727 - val_loss: 1.1244 - val_mean_squared_error: 1.1244 - lr: 0.0010 - test_mse: 1.1997\n",
      "Epoch 48/100\n",
      "439/441 [============================>.] - ETA: 0s - loss: 0.2737 - mean_squared_error: 0.2737\n",
      "Test MSE: 1.6309537887573242\n",
      "441/441 [==============================] - 2s 3ms/step - loss: 0.2729 - mean_squared_error: 0.2729 - val_loss: 1.1856 - val_mean_squared_error: 1.1856 - lr: 0.0010 - test_mse: 1.6310\n",
      "Epoch 49/100\n",
      "441/441 [==============================] - ETA: 0s - loss: 0.2706 - mean_squared_error: 0.2706\n",
      "Test MSE: 1.5650790929794312\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2706 - mean_squared_error: 0.2706 - val_loss: 1.1760 - val_mean_squared_error: 1.1760 - lr: 0.0010 - test_mse: 1.5651\n",
      "Epoch 50/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2794 - mean_squared_error: 0.2794\n",
      "Test MSE: 1.8805805444717407\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2749 - mean_squared_error: 0.2749 - val_loss: 1.2048 - val_mean_squared_error: 1.2048 - lr: 0.0010 - test_mse: 1.8806\n",
      "Epoch 51/100\n",
      "433/441 [============================>.] - ETA: 0s - loss: 0.2648 - mean_squared_error: 0.2648\n",
      "Test MSE: 0.8772016167640686\n",
      "441/441 [==============================] - 1s 3ms/step - loss: 0.2627 - mean_squared_error: 0.2627 - val_loss: 1.1569 - val_mean_squared_error: 1.1569 - lr: 0.0010 - test_mse: 0.8772\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 40)                9760      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,801\n",
      "Trainable params: 9,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{'num_layers': 1, 'learning_rate': 0.01, 'optimizer': 'adam', 'activation': 'relu', 'num_units_0': 40, 'dropout_rate_0': 0.1, 'batch_size': 64, 'tuner/epochs': 34, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n",
      "-------------------------------------\n",
      "Model saved to /Users/junxingli/Desktop/master_thesis/models/lstm/whole-V4/best_model\n"
     ]
    }
   ],
   "source": [
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        x, y = self.test_data\n",
    "        results = self.model.evaluate(x, y, verbose=0)\n",
    "        logs['test_mse'] = results if isinstance(results, float) else results[1]\n",
    "        print('\\nTest MSE:', logs['test_mse'])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='mean_squared_error', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "\n",
    "stop_early = EarlyStopping(monitor='mean_squared_error', \n",
    "                                              patience=20)\n",
    "\n",
    "test_callback = TestCallback((X_test_seq, y_test_seq))\n",
    "\n",
    "best_model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error'])\n",
    "\n",
    "History = best_model.fit(X_train_seq, \n",
    "                         y_train_seq, \n",
    "                         epochs=100, \n",
    "                         callbacks=[stop_early, reduce_lr, test_callback],\n",
    "                         validation_split=0.25)\n",
    "\n",
    "best_model.summary()\n",
    "print(f\"\"\"{best_hps.values}\n",
    "-------------------------------------\n",
    "Model saved to {best_model_path}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if overwrite or not os.path.exists(best_model_path):\n",
    "    print(f\"Model was saved to {best_model_path}\")\n",
    "    best_model.save(best_model_path)\n",
    "else:\n",
    "    print(f\"Model file already exists at {best_model_path}. Set 'overwrite=True' to overwrite the file.\")\n",
    "    \n",
    "utils.plot_mse_over_epochs(History, plots_path, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_df(model, scaler, X_seq, y_seq, times):\n",
    "    y_pred = model.predict(X_seq)\n",
    "    y_pred = scaler.inverse_transform(y_pred).reshape(-1)\n",
    "    y_actual = scaler.inverse_transform(y_seq).reshape(-1)\n",
    "\n",
    "    df_res = pd.DataFrame({'Time': times, \n",
    "                           'Actual': y_actual, \n",
    "                           'Predicted': y_pred})\n",
    "    df_res.sort_values('Time')\n",
    "    \n",
    "    rmse = mean_squared_error(df_res['Actual'], df_res['Predicted'], squared=False)\n",
    "    r2 = r2_score(df_res['Actual'], df_res['Predicted'])\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    \n",
    "    return df_res, rmse\n",
    "\n",
    "loaded_model = keras.models.load_model(best_model_path)\n",
    "df_train, rmse_train = prediction_df(loaded_model, scaler, X_train_seq, y_train_seq, times_train)\n",
    "df_test, rmse_test = prediction_df(loaded_model, scaler, X_test_seq, y_test_seq, times_test)\n",
    "\n",
    "utils.plot_time_predictions(df_train, df_test, plots_path, overwrite=False, limit=[3.5, 4.4], linewidth=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.make_evaluation_plots(df_train,\n",
    "                            df_test,\n",
    "                            'LSTM-whole-data',\n",
    "                            plots_path,\n",
    "                            overwrite=False,\n",
    "                            limit=[3.4, 5],\n",
    "                            error_line=0.05,\n",
    "                            res_limit=[-0.8, 0.6],\n",
    "                            mean=pd.read_pickle(\"../data/processed/SRD_Lysekil.pkl\")['SRD'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XingsKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
